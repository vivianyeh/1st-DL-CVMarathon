{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(\"model_data/yolo.h5\"):\n",
    "  # 下載 yolov3 的網路權重，並且把權重轉換為 keras 能夠讀取的格式\n",
    "  print(\"Model doesn't exist, downloading...\")\n",
    "  os.system(\"wget https://pjreddie.com/media/files/yolov3.weights\")\n",
    "  print(\"Converting yolov3.weights to yolo.h5...\")\n",
    "  os.system(\"python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5\")\n",
    "else:\n",
    "  print(\"Model exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 annotation 轉換訓練時需要的資料形態\n",
    "import xml.etree.ElementTree as ET # 載入能夠 Parser xml 文件的 library\n",
    "def convert_annotation(class_name, image_id):\n",
    "    file_path = f'dataset/{class_name}/annotations/{image_id}.xml'\n",
    "    img_path = f'dataset/{class_name}/images/{image_id}.jpg'\n",
    "    in_file = open(file_path)\n",
    "    \n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    all_txt = ''\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in classes or int(difficult)==1:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
    "        txt = \"./\"+img_path + \" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id)\n",
    "        txt2 = \",\".join([str(a) for a in b]) + ',' + str(cls_id)\n",
    "        if len(all_txt)>0:\n",
    "            all_txt+=' '+txt2\n",
    "        else:\n",
    "            all_txt+=txt\n",
    "    return all_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D50_train0308.txt exist\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "classes = ['raccoon', 'kangaroo', 'kangaroo']\n",
    "dirs = ['raccoon', 'kangaroo','k']\n",
    "trian_file = 'D50_train0308.txt'\n",
    "all_txt = ''\n",
    "if not os.path.exists(trian_file): \n",
    "    list_file = open(trian_file, 'a')\n",
    "    for i in range(len(classes)):\n",
    "        for files in os.walk(f'dataset/{dirs[i]}/images'):\n",
    "            all_files = files[2]\n",
    "            for filename in all_files:\n",
    "                filename = filename.split('.')[0]\n",
    "                all_txt += f'{convert_annotation(dirs[i], filename)}\\n'\n",
    "    list_file.write(all_txt)\n",
    "else:\n",
    "    print(\"D50_train0308.txt exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"model_data/yolo_weights.h5\"):\n",
    "  print(\"Converting pretrained YOLOv3 weights for training\")\n",
    "  os.system(\"python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5\") \n",
    "else:\n",
    "  print(\"Pretrained weights exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 將 train.py 所需要的套件載入\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_classes, get_anchors, create_model, create_tiny_model, data_generator, data_generator_wrapper\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "CUDA_VISIBLE_DEVICES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = 'D50_train0308.txt' # 轉換好格式的標註檔案\n",
    "log_dir = 'logs/0038/' # 訓練好的模型儲存的路徑\n",
    "classes_path = 'model_data/D50_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "anchors = get_anchors(anchors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 320)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分為 training 以及 validation\n",
    "val_split = 0.2\n",
    "with open(annotation_path) as f:\n",
    "    lines = f.readlines()\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "num_val = int(len(lines)*val_split)\n",
    "num_train = len(lines) - num_val\n",
    "(num_val, num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create YOLOv3 model with 9 anchors and 2 classes.\n",
      "Load weights model_data/yolo_weights.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n"
     ]
    }
   ],
   "source": [
    "input_shape = (416,416) # multiple of 32, hw\n",
    "\n",
    "is_tiny_version = len(anchors)==6 # default setting\n",
    "if is_tiny_version:\n",
    "    model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "        freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n",
    "else:\n",
    "    model = create_model(input_shape, anchors, num_classes,\n",
    "        freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze\n",
    "\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "    monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, val on 80 samples, with batch size 2.\n",
      "Epoch 1/50\n",
      "160/160 [==============================] - 42s 263ms/step - loss: 636.5700 - val_loss: 71.3538\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 38s 236ms/step - loss: 58.7468 - val_loss: 39.3061\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 38s 237ms/step - loss: 38.1923 - val_loss: 30.4636\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 37s 234ms/step - loss: 30.9946 - val_loss: 26.4833\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 37s 231ms/step - loss: 27.3244 - val_loss: 24.1052\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 24.8450 - val_loss: 22.1191\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 23.4803 - val_loss: 21.1333\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 22.3561 - val_loss: 20.5589\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 37s 233ms/step - loss: 21.9822 - val_loss: 19.3108\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 37s 233ms/step - loss: 21.3350 - val_loss: 20.5942\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 37s 229ms/step - loss: 20.9346 - val_loss: 20.3977\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 36s 228ms/step - loss: 20.2447 - val_loss: 19.5639\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 36s 228ms/step - loss: 19.9057 - val_loss: 18.6397\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 37s 228ms/step - loss: 19.5478 - val_loss: 18.1188\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 36s 228ms/step - loss: 19.2518 - val_loss: 19.1895\n",
      "Epoch 16/50\n",
      "160/160 [==============================] - 37s 229ms/step - loss: 19.0358 - val_loss: 17.9150\n",
      "Epoch 17/50\n",
      "160/160 [==============================] - 37s 234ms/step - loss: 18.9090 - val_loss: 19.1072\n",
      "Epoch 18/50\n",
      "160/160 [==============================] - 38s 237ms/step - loss: 19.0845 - val_loss: 17.8312\n",
      "Epoch 19/50\n",
      "160/160 [==============================] - 38s 236ms/step - loss: 18.7209 - val_loss: 18.4044\n",
      "Epoch 20/50\n",
      "160/160 [==============================] - 37s 234ms/step - loss: 18.3809 - val_loss: 18.4704\n",
      "Epoch 21/50\n",
      "160/160 [==============================] - 38s 235ms/step - loss: 18.1698 - val_loss: 17.9104\n",
      "Epoch 22/50\n",
      "160/160 [==============================] - 37s 231ms/step - loss: 18.1064 - val_loss: 17.5498\n",
      "Epoch 23/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 18.2588 - val_loss: 17.0173\n",
      "Epoch 24/50\n",
      "160/160 [==============================] - 37s 231ms/step - loss: 18.3152 - val_loss: 17.6028\n",
      "Epoch 25/50\n",
      "160/160 [==============================] - 37s 231ms/step - loss: 17.9648 - val_loss: 17.9071\n",
      "Epoch 26/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.9528 - val_loss: 17.5146\n",
      "Epoch 27/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.7169 - val_loss: 17.9291\n",
      "Epoch 28/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.7427 - val_loss: 18.3734\n",
      "Epoch 29/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.8883 - val_loss: 17.6491\n",
      "Epoch 30/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.7175 - val_loss: 17.1600\n",
      "Epoch 31/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.5369 - val_loss: 17.0107\n",
      "Epoch 32/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.8124 - val_loss: 17.7985\n",
      "Epoch 33/50\n",
      "160/160 [==============================] - 37s 233ms/step - loss: 17.5115 - val_loss: 17.9718\n",
      "Epoch 34/50\n",
      "160/160 [==============================] - 37s 231ms/step - loss: 17.5634 - val_loss: 17.1758\n",
      "Epoch 35/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.8116 - val_loss: 18.1017\n",
      "Epoch 36/50\n",
      "160/160 [==============================] - 37s 231ms/step - loss: 18.0749 - val_loss: 18.5302\n",
      "Epoch 37/50\n",
      "160/160 [==============================] - 37s 234ms/step - loss: 17.4481 - val_loss: 17.4684\n",
      "Epoch 38/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.5855 - val_loss: 17.0312\n",
      "Epoch 39/50\n",
      "160/160 [==============================] - 38s 235ms/step - loss: 17.7961 - val_loss: 17.0391\n",
      "Epoch 40/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.3910 - val_loss: 17.6538\n",
      "Epoch 41/50\n",
      "160/160 [==============================] - 37s 234ms/step - loss: 17.6409 - val_loss: 17.2374\n",
      "Epoch 42/50\n",
      "160/160 [==============================] - 38s 237ms/step - loss: 17.2119 - val_loss: 17.3936\n",
      "Epoch 43/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.3532 - val_loss: 17.3948\n",
      "Epoch 44/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.5708 - val_loss: 17.7120\n",
      "Epoch 45/50\n",
      "160/160 [==============================] - 37s 232ms/step - loss: 17.4273 - val_loss: 17.3743\n",
      "Epoch 46/50\n",
      "160/160 [==============================] - 37s 233ms/step - loss: 17.4567 - val_loss: 17.0810\n",
      "Epoch 47/50\n",
      "160/160 [==============================] - 37s 231ms/step - loss: 17.2854 - val_loss: 16.8828\n",
      "Epoch 48/50\n",
      "160/160 [==============================] - 38s 236ms/step - loss: 17.3031 - val_loss: 17.8055\n",
      "Epoch 49/50\n",
      "160/160 [==============================] - 38s 237ms/step - loss: 17.0565 - val_loss: 18.0422\n",
      "Epoch 50/50\n",
      "160/160 [==============================] - 38s 236ms/step - loss: 17.4304 - val_loss: 17.3010\n",
      "Unfreeze all of the layers.\n",
      "Train on 320 samples, val on 80 samples, with batch size 2.\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 124s 775ms/step - loss: 18.0129 - val_loss: 17.9077\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 110s 691ms/step - loss: 16.7744 - val_loss: 16.4519\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 112s 697ms/step - loss: 16.4978 - val_loss: 15.8238\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 111s 695ms/step - loss: 16.5736 - val_loss: 17.5589\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 111s 696ms/step - loss: 16.2838 - val_loss: 15.8319\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 112s 702ms/step - loss: 15.5568 - val_loss: 16.6991\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 112s 701ms/step - loss: 15.1653 - val_loss: 14.8395\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 111s 696ms/step - loss: 14.5781 - val_loss: 14.9707\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 111s 692ms/step - loss: 14.3878 - val_loss: 14.6008\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 111s 694ms/step - loss: 14.0637 - val_loss: 14.6286\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 111s 691ms/step - loss: 13.9929 - val_loss: 14.9472\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 111s 691ms/step - loss: 13.9232 - val_loss: 15.0703\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 111s 691ms/step - loss: 14.0678 - val_loss: 14.4587\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 111s 691ms/step - loss: 13.9121 - val_loss: 14.0627\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 111s 691ms/step - loss: 13.9473 - val_loss: 14.4787\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 111s 695ms/step - loss: 14.0919 - val_loss: 14.6202\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 112s 697ms/step - loss: 14.0636 - val_loss: 15.3623\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 112s 701ms/step - loss: 13.9806 - val_loss: 14.4379\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 112s 698ms/step - loss: 13.8838 - val_loss: 14.2645\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 111s 692ms/step - loss: 13.7925 - val_loss: 14.1183\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 111s 691ms/step - loss: 13.7515 - val_loss: 14.2590\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 111s 693ms/step - loss: 13.6465 - val_loss: 14.4914\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 112s 697ms/step - loss: 14.0378 - val_loss: 14.3770\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 111s 691ms/step - loss: 14.0390 - val_loss: 14.9993\n",
      "Epoch 00074: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train with frozen layers first, to get a stable loss.\n",
    "# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "# 一開始先 freeze YOLO 除了 output layer 以外的 darknet53 backbone 來 train\n",
    "if True:\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "        # use custom yolo_loss Lambda layer.\n",
    "        'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "    batch_size = 2\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    # 模型利用 generator 產生的資料做訓練，強烈建議大家去閱讀及理解 data_generator_wrapper 在 train.py 中的實現\n",
    "    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=50,\n",
    "            initial_epoch=0,\n",
    "            callbacks=[logging, checkpoint])\n",
    "    model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
    "\n",
    "# Unfreeze and continue training, to fine-tune.\n",
    "# Train longer if the result is not good.\n",
    "if True:\n",
    "    # 把所有 layer 都改為 trainable\n",
    "    for i in range(len(model.layers)):\n",
    "        model.layers[i].trainable = True\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "    print('Unfreeze all of the layers.')\n",
    "\n",
    "    batch_size = 2 # note that more GPU memory is required after unfreezing the body\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "        steps_per_epoch=max(1, num_train//batch_size),\n",
    "        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "        validation_steps=max(1, num_val//batch_size),\n",
    "        epochs=100,\n",
    "        initial_epoch=50,\n",
    "        callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
    "    model.save_weights(log_dir + 'trained_weights_final.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
